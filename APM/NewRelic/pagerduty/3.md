how does the notifcation looks like and what thet tell on call
# PagerDuty Notification Content and Structure Guide

The content and clarity of the notification are what make PagerDuty effective. A good alert tells the on-call engineer what is wrong, how bad it is, and where to look immediately.

Here's how the notifications look and what they tell the on-call person.

## 1. Push Notification (Mobile App)

This is usually the first alert. It's designed to be glanceable.

### What it looks like:
A standard phone push notification.

### What it tells you:
- **Title:** The name of the triggered alert policy (e.g., `CRITICAL: High Error Rate`)
- **Body:** The name of the service/entity affected (e.g., `order-service`)
- **Priority:** It will clearly show the priority level (e.g., `P1` or `Critical`)

**What the on-call thinks:** "Something is critically wrong with order-service. I need to look at this now."

## 2. Phone Call (Voice Alert)

If the push notification isn't acknowledged, PagerDuty calls. This is a computer-generated voice that reads the alert details.

### What it sounds like:

> "Hello, this is PagerDuty with a critical priority alert. Alert title: C R I T I C A L, colon, High Error Rate. For service: order-service. To acknowledge this incident, press 4. To hear details again, press 5."

**What the on-call thinks:** "This is serious. I need to acknowledge this call to stop the escalation and then immediately find a computer to investigate."

### Key Action:
The engineer presses `4` on their phone to acknowledge the incident. This tells PagerDuty, "I'm on it," and stops the notification sequence for them (though the incident remains open until resolved).

## 3. SMS (Text Message)

The SMS provides more detail than the push notification and is useful for a quick view without opening the app.

### What it looks like:

```text
PagerDuty: [P1] Incident #123 triggered
Alert: CRITICAL: High Error Rate
Service: order-service
Details: https://your-company.pagerduty.com/incidents/ABC123
```

**What the on-call thinks:** "The direct link to the incident is right here. I can click it to start investigating."

## 4. The PagerDuty Incident Page (The "Command Center")

This is the most important part. When the on-call engineer clicks the link (in the SMS or app), they are taken to the incident page. This page is designed to give them all the context they need to start debugging immediately.

### What it tells the on-call:

#### Basic Information:
- **Title & Priority:** Clear statement of the problem (e.g., `[P1] CRITICAL: High Error Rate on order-service`)
- **Status:** Is it Triggered (no one working on it), Acknowledged (someone is on it), or Resolved?

#### The Original Alert Description:
This is the rich information sent from New Relic. It often includes:

- A direct link to the relevant New Relic dashboard for this alert
- A link to the specific error trace in New Relic
- The NRQL query that triggered the alert, showing the exact graph that breached the threshold
- Custom summary fields you can configure in New Relic (e.g., `Error Message: Database connection timeout`, `Node: k8s-node-7b41`)

#### Context Information:
- **Service & Escalation Policy:** Which service is affected and who is currently on call for it
- **Activity Log:** A full timeline of the incident: when it was triggered, who was notified, who acknowledged it, and any notes they added

## What a Well-Configured Alert from New Relic Looks Like

A bad alert just says "Something is wrong." A good alert, configured correctly in New Relic's alert condition, provides context. The on-call should not have to guess.

### Example of a Good Alert Description (configured in New Relic):

```text
[CRITICAL] High Error Rate detected on {{replacementString}}.
• Service: {{tags.appName}}
• Error Count: {{value}}
• Primary Error Message: {{description}}
• View the investigation dashboard: https://example.com/dashboard?id=123
• Direct link to traces: https://example.com/error-traces?appName={{tags.appName}}
```

### What the on-call engineer sees and does:

1. **Gets the push/call:** "CRITICAL: High Error Rate on order-service"

2. **Opens the PagerDuty incident:** Sees the details above

3. **Immediate Action:** They click the "Direct link to traces" URL. It takes them directly to the New Relic Error Inbox or Distributed Tracing view, filtered for order-service, showing them the exact errors that are happening right now.

4. **Diagnosis:** They see the stack trace, see the error is `Database connection timeout`, and can immediately start investigating the database or connection pool.

## Summary: What a Good Notification Provides

A good PagerDuty notification, powered by a well-configured New Relic alert, tells the on-call engineer:

| Element | Information |
|---------|------------|
| **What** | `CRITICAL: High Error Rate` |
| **Where** | `order-service / production` |
| **How Bad** | `P1 priority, 1000 errors/minute` |
| **Where to Look** | Direct links to the relevant dashboards and logs in New Relic |
| **What to Do** | Acknowledge the alert, investigate using the provided links, collaborate with the team if needed |

## Why This Matters

This context is critical for reducing **Mean Time To Resolution (MTTR)** and preventing **alert fatigue**, as engineers aren't wasting time just trying to find the source of the problem.

### Key Benefits:

- **Immediate context** - No guessing what's wrong
- **Direct access** - Links straight to investigation tools  
- **Clear severity** - Priority levels guide response urgency
- **Actionable information** - Specific details enable quick diagnosis
- **Reduced cognitive load** - All necessary information in one place

This comprehensive notification system transforms alerts from simple notifications into actionable intelligence that enables rapid incident response and resolution.