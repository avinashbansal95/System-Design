you guided me about setting up new relic with node.js in above chat
now show mw how you can integrate slack and pugerduty?
is slack and pagerduty integreates with newrelic or with an independent node.js application?
i would prefer with newrelic if there is an option
also explain what pagerduty and slack would be helpful for?
how to generate alarms using these


# New Relic Alerting Integration Guide - PagerDuty & Slack

This moves from building the system to making it observable and operable, which is crucial for any production-grade application.

## What are PagerDuty and Slack For?

### Slack: For Collaboration and Visibility
- **Purpose:** Non-critical alerts, notifications, and general system status updates
- **Goal:** Keep the team informed
- **Example:** "Warning: CPU usage on user-service is above 70%"

### PagerDuty: For Alerting and On-Call Management
- **Purpose:** Critical alerts that require immediate human intervention
- **Goal:** Ensure the right person is notified through multiple channels (phone call, SMS, push notification) until acknowledged
- **Example:** "CRITICAL: order-service is down! Error rate is 100%"

> **In short:** Slack informs, PagerDuty alarms.

## How Do They Integrate? New Relic vs. Independent App

**Answer:** They integrate directly with New Relic. You do **not** need to build a custom Node.js application for this core functionality.

New Relic has built-in **Notification Channels** and a powerful **Workflows** tool that allows you to route specific alerts to specific destinations like Slack and PagerDuty.

### Data Flow

1. **Your Node.js app** sends metrics/errors to New Relic
2. **New Relic evaluates** the alerting conditions you set up (e.g., error rate > 5% for 5 minutes)
3. **When a condition is violated**, New Relic triggers an Incident
4. **This incident is sent to PagerDuty** via a direct integration, which then pages the on-call engineer
5. **The same incident** (or a less critical one) can also be sent to a Slack channel for visibility

## Step-by-Step Guide to Integration

### Prerequisites
- A New Relic account with your application sending data (as we set up previously)
- A Slack workspace where you have permissions to add apps
- A PagerDuty account (you can start with a free trial)

## Part 1: Integrating Slack with New Relic

**Goal:** Get non-critical alerts and notifications into a Slack channel.

### In Slack:
1. Create a new channel (e.g., `#infra-alerts`)

### In New Relic:
1. Go to **Alerts & AI → Notification channels**
2. Click **Add channel**
3. Select **Slack**
4. Click **Add to Slack** (this will redirect you to Slack to authorize the New Relic app)
5. Choose the Slack channel you created (`#infra-alerts`) and select the workspace
6. Complete the authorization (you'll be redirected back to New Relic)
7. **Test the connection** - New Relic will send a test message to the Slack channel

## Part 2: Integrating PagerDuty with New Relic

**Goal:** Get critical alerts to page the on-call engineer.

### In PagerDuty:
1. Create a **Service** (e.g., "E-Commerce Platform Production")
2. Inside the service, go to **Integrations → Add a new integration**
3. Choose the **New Relic** integration type
4. PagerDuty will provide you with an **Integration Key** (a long string of letters and numbers)
5. **Copy this key**

### In New Relic:
1. Go to **Alerts & AI → Notification channels**
2. Click **Add channel**
3. Select **PagerDuty**
4. Give it a name (e.g., "Prod PagerDuty")
5. Paste the **Integration Key** you copied from PagerDuty
6. Save the channel

## Part 3: Creating Alert Policies and Routing with Workflows

This is the most important part. Now you define what triggers an alert and where it gets sent.

### Create a Critical Alert Policy (for PagerDuty):
1. In New Relic, go to **Alerts & AI → Alert Policies → Create a new policy**
2. Name it something like "Critical Production Errors"
3. Under **Incident preferences**, select **CRITICAL** (this is key for PagerDuty integration)
4. Go to the **Workflows** tab for this policy
5. Click **Add destination** and select your PagerDuty notification channel
6. This means any incident in this policy will go to PagerDuty

### Create a Warning Alert Policy (for Slack):
1. Create another policy named "Warning Production Metrics"
2. Under **Incident preferences**, select **WARNING**
3. In the **Workflows** tab, add your Slack notification channel

### Create Alert Conditions (The actual alarms):

Now, add conditions to your policies. A condition is the specific rule that triggers the alarm.

#### Example CRITICAL Condition (for PagerDuty):
1. Navigate to your "Critical Production Errors" policy
2. Click **Create condition** → Choose **NRQL** (this is New Relic's query language)
3. Write a query to monitor for total errors:

```sql
SELECT count(*) FROM TransactionError WHERE appName = 'Your_Nodejs_App_Name' FACET error.message
```

4. Set the threshold: **Static → above → 100** (in the last 5 minutes)
   - This means if there are more than 100 errors in 5 min, it's CRITICAL
5. Save the condition. Now, this will trigger a PagerDuty incident

#### Example WARNING Condition (for Slack):
1. Navigate to your "Warning Production Metrics" policy
2. **Create condition → NRQL**
3. Write a query to monitor high latency:

```sql
SELECT percentile(duration, 95) FROM Transaction WHERE appName = 'Your_Nodejs_App_Name'
```

4. Set the threshold: **above 2 seconds** (in the last 10 minutes)
   - This means if the 95th percentile latency is above 2s for 10 min, it's a WARNING
5. Save. This will send a message to Slack

## Putting It All Together: A Practical Scenario

Here's how this integrated system works in practice:

### The Incident
Your `order-service` starts throwing errors due to a database connection leak.

### The Response Chain

1. **After 3 minutes**, the error count crosses 100. New Relic detects this.

2. **New Relic creates a CRITICAL incident** in the "Critical Production Errors" policy.

3. **Because this policy is linked to PagerDuty**, PagerDuty triggers a high-urgency incident and starts calling/SMSing the on-call engineer based on their schedule.

4. **Simultaneously**, the high error rate causes latency to spike to 3 seconds.

5. **After 10 minutes of high latency**, New Relic triggers a WARNING incident in the other policy.

6. **This warning is sent to the #infra-alerts Slack channel**, giving the rest of the team context about what the on-call engineer is investigating.

7. **The on-call engineer**:
   - Acknowledges the page in PagerDuty
   - Joins a Zoom war room
   - Uses the links in the PagerDuty alert (which point to New Relic dashboards)
   - Identifies the faulty deployment
   - Rolls it back

8. **Once the error count drops below the threshold**, New Relic automatically resolves the incident, which PagerDuty relays, letting everyone know the issue is fixed.

## Summary

This integrated setup creates a powerful, automated safety net for your application, ensuring that:

- ✅ **Problems are detected automatically**
- ✅ **The right people are notified immediately**  
- ✅ **Context and tools are provided for quick resolution**
- ✅ **Team visibility is maintained throughout the incident**
- ✅ **Resolution is automatically communicated**

This approach scales from small teams to large enterprises and is considered a production-grade standard for modern applications.